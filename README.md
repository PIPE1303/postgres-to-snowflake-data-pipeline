# ğŸš€ PostgreSQL to Snowflake Data Pipeline

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8+-green.svg)](https://airflow.apache.org)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://postgresql.org)
[![Snowflake](https://img.shields.io/badge/Snowflake-Data%20Warehouse-orange.svg)](https://snowflake.com)
[![AWS S3](https://img.shields.io/badge/AWS%20S3-Storage-yellow.svg)](https://aws.amazon.com/s3)

A comprehensive and automated data pipeline that orchestrates ETL processes between PostgreSQL databases (Supabase & RDS), AWS S3, and Snowflake, featuring automatic data dictionary generation and comprehensive documentation. This enterprise-grade solution implements modern data engineering practices with Bronze and Silver data layers, timezone-aware processing, and automated metadata management.

## ğŸ—ï¸ Arquitectura

```
PostgreSQL (Supabase + RDS) â†’ S3 (Parquet) â†’ Snowflake (Bronze) â†’ Snowflake (Silver) â†’ Supabase (Reports)
```

### Componentes Principales

- **ğŸ”§ Apache Airflow**: OrquestaciÃ³n de workflows
- **ğŸ˜ PostgreSQL**: Bases de datos fuente (Supabase, RDS)
- **â˜ï¸ AWS S3**: Almacenamiento intermedio (formato Parquet)
- **â„ï¸ Snowflake**: Data warehouse (capas Bronze y Silver)
- **ğŸ“Š Python**: Scripts de procesamiento y transformaciÃ³n
- **ğŸ“š DocumentaciÃ³n**: GeneraciÃ³n automÃ¡tica de diccionarios de datos

## ğŸš€ CaracterÃ­sticas

### âœ¨ Funcionalidades Principales

- **ğŸ”„ Pipeline ETL Completo**: ExtracciÃ³n, transformaciÃ³n y carga automatizada
- **ğŸ“ˆ Capas de Datos**: ImplementaciÃ³n de arquitectura Bronze â†’ Silver
- **ğŸŒ Zona Horaria**: ConfiguraciÃ³n para Colombia (UTC-5)
- **ğŸ“‹ Metadatos**: Comentarios automÃ¡ticos en tablas y columnas
- **ğŸ“– Diccionarios**: GeneraciÃ³n automÃ¡tica de documentaciÃ³n
- **ğŸ§¹ Limpieza**: GestiÃ³n automÃ¡tica de archivos temporales
- **â˜ï¸ S3 Integration**: Subida automÃ¡tica de diccionarios a S3

### ğŸ¯ DAGs Disponibles

| DAG | DescripciÃ³n | Frecuencia |
|-----|-------------|------------|
| `data_pipeline_variables_dag` | CreaciÃ³n de esquemas y carga de datos | Manual |
| `export_to_s3_dag` | ExportaciÃ³n PostgreSQL â†’ S3 (Parquet) | Manual |
| `s3_to_snowflake_dag` | Carga S3 â†’ Snowflake (Bronze Layer) | Manual |
| `silver_layer_dag` | GeneraciÃ³n de capa Silver + Reportes | Manual |
| `data_dictionary_dag` | GeneraciÃ³n de diccionarios de datos | Semanal |

## ğŸ“ Estructura del Proyecto

```
postgres-to-snowflake-data-pipeline/
â”œâ”€â”€ ğŸ“ dags/                          # DAGs de Airflow
â”‚   â”œâ”€â”€ data_pipeline_variables_dag.py
â”‚   â”œâ”€â”€ export_to_s3_dag.py
â”‚   â”œâ”€â”€ s3_to_snowflake_dag.py
â”‚   â”œâ”€â”€ silver_layer_dag.py
â”‚   â””â”€â”€ data_dictionary_dag.py
â”œâ”€â”€ ğŸ“ scripts/                       # Scripts de procesamiento
â”‚   â”œâ”€â”€ create_schema_with_metadata.py
â”‚   â”œâ”€â”€ load_data.py
â”‚   â”œâ”€â”€ export_to_s3.py
â”‚   â”œâ”€â”€ generate_postgres_dictionary.py
â”‚   â”œâ”€â”€ generate_snowflake_dictionary.py
â”‚   â””â”€â”€ generate_markdown_docs.py
â”œâ”€â”€ ğŸ“ data/                          # Datos de ejemplo
â”‚   â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ rds/
â”œâ”€â”€ ğŸ“ docs/                          # DocumentaciÃ³n
â”‚   â”œâ”€â”€ README_DICTIONARIES.md
â”‚   â””â”€â”€ README_METADATA.md
â”œâ”€â”€ ğŸ“ .cursor/                       # Reglas de Cursor AI
â”‚   â””â”€â”€ rules/
â”œâ”€â”€ ğŸ³ Dockerfile                     # ConfiguraciÃ³n Docker
â”œâ”€â”€ ğŸ“‹ requirements.txt               # Dependencias Python
â”œâ”€â”€ ğŸ“¦ packages.txt                   # Paquetes del sistema
â””â”€â”€ âš™ï¸ airflow_settings.yaml         # ConfiguraciÃ³n Airflow
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Docker y Docker Compose
- Python 3.12+
- Acceso a PostgreSQL (Supabase, RDS)
- Acceso a Snowflake
- Acceso a AWS S3

### 1. Clonar el Repositorio

```bash
git clone https://github.com/PIPE1303/postgres-to-snowflake-data-pipeline.git
cd postgres-to-snowflake-data-pipeline
```

### 2. Configurar Variables de Airflow

```bash
# Variables de PostgreSQL
airflow variables set supabase_host "tu-supabase-host"
airflow variables set supabase_name "tu-supabase-db"
airflow variables set supabase_user "tu-usuario"
airflow variables set supabase_password "tu-password"
airflow variables set supabase_port "5432"

airflow variables set rds_host "tu-rds-host"
airflow variables set rds_name "tu-rds-db"
airflow variables set rds_user "tu-usuario"
airflow variables set rds_password "tu-password"
airflow variables set rds_port "5432"

# Variables de AWS S3
airflow variables set S3_BUCKET "tu-bucket-name"

# Variables de Snowflake
airflow variables set SNOWFLAKE_STAGE "@GENNIUS_XYZ.COMPANY_BRONZE_LAYER.S3_STAGE"
```

### 3. Configurar Conexiones de Airflow

```bash
# ConexiÃ³n PostgreSQL Supabase
airflow connections add supabase_default \
    --conn-type postgres \
    --conn-host tu-supabase-host \
    --conn-schema tu-supabase-db \
    --conn-login tu-usuario \
    --conn-password tu-password \
    --conn-port 5432

# ConexiÃ³n PostgreSQL RDS
airflow connections add rds_default \
    --conn-type postgres \
    --conn-host tu-rds-host \
    --conn-schema tu-rds-db \
    --conn-login tu-usuario \
    --conn-password tu-password \
    --conn-port 5432

# ConexiÃ³n Snowflake
airflow connections add snowflake_default \
    --conn-type snowflake \
    --conn-login tu-usuario \
    --conn-password tu-password \
    --conn-extra '{"account": "tu-account", "warehouse": "COMPUTE_WH", "database": "GENNIUS_XYZ", "schema": "COMPANY_BRONZE_LAYER", "role": "PUBLIC"}'

# ConexiÃ³n AWS
airflow connections add aws_default \
    --conn-type aws \
    --conn-extra '{"aws_access_key_id": "tu-access-key", "aws_secret_access_key": "tu-secret-key", "region_name": "us-east-1"}'
```

### 4. Ejecutar con Docker

```bash
# Construir y ejecutar
docker-compose up -d

# Ver logs
docker-compose logs -f

# Acceder a Airflow UI
# http://localhost:8080 (admin/admin)
```

## ğŸš€ Uso

### EjecuciÃ³n Manual de DAGs

```bash
# 1. Crear esquemas y cargar datos
airflow dags trigger data_pipeline_variables_dag

# 2. Exportar a S3
airflow dags trigger export_to_s3_dag

# 3. Cargar a Snowflake
airflow dags trigger s3_to_snowflake_dag

# 4. Generar capa Silver
airflow dags trigger silver_layer_dag

# 5. Generar diccionarios
airflow dags trigger data_dictionary_dag
```

### Flujo de Datos

1. **ğŸ“Š Carga Inicial**: Datos CSV â†’ PostgreSQL (Supabase + RDS)
2. **ğŸ“¤ ExportaciÃ³n**: PostgreSQL â†’ S3 (formato Parquet)
3. **â„ï¸ Bronze Layer**: S3 â†’ Snowflake (datos raw)
4. **âœ¨ Silver Layer**: Transformaciones y reportes en Snowflake
5. **ğŸ“‹ DocumentaciÃ³n**: GeneraciÃ³n automÃ¡tica de diccionarios

## ğŸ“Š Datos de Ejemplo

El proyecto incluye datos de ejemplo para un sistema de puntos de tarjetas de crÃ©dito:

- **ğŸ¦ Bancos**: InformaciÃ³n de instituciones financieras
- **ğŸ‘¤ Titulares**: Datos de titulares de tarjetas
- **ğŸ’³ Tarjetas**: InformaciÃ³n de tarjetas de puntos
- **ğŸ¯ Programas**: Programas de lealtad
- **ğŸ‘¥ Usuarios**: Datos de usuarios del sistema

## ğŸ“š DocumentaciÃ³n

### Diccionarios de Datos

Los diccionarios se generan automÃ¡ticamente y se suben a S3 en:
```
s3://tu-bucket/metadata/data-dictionaries/YYYY/MM/DD/
â”œâ”€â”€ data_dictionary_summary.md
â”œâ”€â”€ postgres_unified_dictionary.json
â”œâ”€â”€ snowflake_dictionary.json
â””â”€â”€ [otros archivos de documentaciÃ³n]
```

### Metadatos

- **Tablas**: Comentarios descriptivos en todas las tablas
- **Columnas**: DescripciÃ³n del propÃ³sito de cada columna
- **Restricciones**: DocumentaciÃ³n de claves primarias y forÃ¡neas
- **Relaciones**: Mapeo de relaciones entre tablas

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Zona Horaria

El sistema estÃ¡ configurado para Colombia (UTC-5):
- Archivos S3 organizados por fecha local
- Timestamps en formato local
- Procesamiento respetando horario comercial

### Formato de Datos

- **S3**: Parquet (optimizado para Snowflake)
- **Snowflake**: TIMESTAMP_NTZ para timestamps
- **PostgreSQL**: Timestamps con timezone

### Seguridad

- Credenciales gestionadas por Airflow Connections
- Variables sensibles encriptadas
- Acceso basado en roles (Snowflake)

## ğŸ§ª Testing

```bash
# Ejecutar tests
python -m pytest tests/

# Test de DAGs
airflow dags test data_pipeline_variables_dag 2025-01-01
```

## ğŸ“ˆ Monitoreo

### Logs

- **Airflow UI**: Monitoreo de DAGs y tareas
- **S3**: Logs de procesamiento
- **Snowflake**: Query history y performance

### MÃ©tricas

- Tiempo de ejecuciÃ³n de DAGs
- Volumen de datos procesados
- Errores y reintentos
- Uso de recursos

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¥ Autores

- **AndrÃ©s Felipe Marciales Pardo** - *Desarrollo inicial* - [@PIPE1303](https://github.com/PIPE1303)
- **LinkedIn**: [AndrÃ©s Marciales](https://www.linkedin.com/in/andres-marciales-de/)

## ğŸ™ Agradecimientos

- Apache Airflow Community
- Snowflake Documentation
- PostgreSQL Community
- AWS Documentation

## ğŸ“ Soporte

Para soporte tÃ©cnico o preguntas:
- ğŸ“§ Email: amarciales56@gmail.com
- ğŸ› Issues: [GitHub Issues](https://github.com/PIPE1303/postgres-to-snowflake-data-pipeline/issues)
- ğŸ“– Wiki: [DocumentaciÃ³n completa](https://github.com/PIPE1303/postgres-to-snowflake-data-pipeline/wiki)

---

â­ **Â¡Si este proyecto te fue Ãºtil, no olvides darle una estrella!** â­